codices.token:CONTINUUM;
Lets us know the structure of a token, specifically, type and value (.string)
codices.tokenize:CONTINUUM;
Tokenizes a text stream in-line with Python  syntax
codices:THROUGHLINE;
Behind all the metaphor, we are parsing Python script files—
using the standard `tokenize` library to extract symbolic structure from source code.

Tokenization gives us a symbolic view of the source, simplifying parsing.
It turns complex syntax into a simple (though intermediate) lexicon.

So:
    def parse(source):
becomes:
    NAME NAME OP NAME OP OP

Tokens have limited understanding of their role, but great clarity about what they are.
For example, a token knows that 'def' is a NAME, but not that it's a Python reserved word.

Meaning emerges through layered symbolism.

So:
    NAME NAME OP NAME OP OP
might, if parsing for execution, become:
    keyword identity lbrace identity rbrace colon

But we are not parsing to execute — we are parsing to extract a limited set of precise entities:
prescriptive texts and the things they prescribe, within their Pythonic scope.

We do not care about things like braces and colons.

The base CODEX provides the layered symbolism we do care about.
For example, it tells us whether a NAME token is an ENCAPSULATOR —
a symbol that opens a new scope (like `def` or `class`).

In the CODEX metaphor, a CODEX_OBJECT equates to `token.type`.
It maps tokenize type names (e.g. 'NAME') to symbolic object names.

We could have lived without CODEX_OBJECT, but I didn’t like seeing `token.type` constructs scattered throughout the code.

Note: this module fully wraps the TOKEN structure, which is never directly referenced elsewhere.
codices.CODEX_OBJECTS:KNOWLEDGE;
Objects define the nature of subjects. E.g. 'def' is a NAME object
codices.CODEX:AFFORDANCE;
Though literally a tree trunk (from the Latin *codex*), the term has come to signify a book of law — or more precisely, of lore.

Not 'law' in the contemporary sense, but rather 'ritual' or 'rite': the correct sequence of symbols that achieves a result.

And herein, our codices do just that — they prescribe the correct sequence of things in terms of their symbolism (ruinic nature).

In the base CODEX, very little is known beyond the symbolic meaning of what occurs in the provided sequence.

Thus, the base CODEX offers:
- a symbolic lexicon (as a dict),
- a means to generate the sequence (objectify the source),
- and a way to discern the true name (or value) of encountered symbols.
codices.CODEX.LEXICON:KNOWLEDGE;
Symbolises specific python token subjects that direct our layered parsing
codices.CODEX.objectify:MECHANISM;
Encapsulates the tokenisation process
codices.CODEX.token_val:MECHANISM;
We want to contain ALL token structure knowledge to the base CODEX
so here's how we get to what a token actually IS.
codices.CODEX.token_start:MECHANISM;
We want to contain ALL token structure knowledge to the base CODEX
so here's how we get where the token occurred
codices.ENTITY:AFFORDANCE;
Derived codices apply the base CODEX LEXICON through sets of ENTITIES — statements of what we care about, expressed as layered symbolism, in the derived metaphor.

For example, when building a lineage, we want to ignore Python RESERVED words.
We're interested only in NAME tokens that represent actual Pythonic objects (variables, classes, etc).

Reserved words like `def` and `yield` serve as HONOURIFICS—titles that address true identities.

So we might define:
    HONOURIFICS = ENTITY('NAME', 'RESERVED')
E.g. an HONOURIFIC as any NAME in the set of RESERVED values

Entities are compound matching structures — ritual instruments that determine whether a given 'thing' satisfies the lore of the codex.

Each codex offers a distinct lens on shared material, and begins by defining the entities of interest.
This class serves as their oracle, revealing when those entities have been fulfilled — or left wanting.

The 'things' we encounter consist of both type and value.
Entities allow us to match by type alone, or by both type and value.
codices.ENTITY.add:MECHANISM;
Extends the ENTITY's potence by adding more types and values it recognises.
codices.ENTITY.is_entity:SKILL;
Does this ENTITY recognise a given token?
NO if the ENTITY doesn't recognise tokens of this token's type, OTHERWISE
YES if the ENTITY recognises ALL tokens of this token's type, OR ELSE
YES if the ENTITY also recognises this token's value
granulator.os:CONTINUUM;
allows conversion of an underlying filepath to a batch identity
granulator.dataclasses:CONTINUUM;
allows us to create a named structure for the final substance list we produce
granulator:THROUGHLINE;
We pour a batch of Python source code (.py file) into the Granulator.

Through a series of processing steps, we generate an inventory of 'grains'—each providing:
    - lineage: where, in the original bulk material, this grain was found
    - type: whether it is a TEXT description or an object IDENTITY
    - substance: the found TEXT or IDENTITY itself

Metaphorically, the Python source code is the bulk_material we work on.
It is tokenized into a powder of token particles, which are then refined into grains.

Particles are tracked through a batch record, capturing the Pythonic scope in which each particle is found.

Particles are distilled into grains such that a sequence like:
    NAME.string='self' OP.string='.' NAME.string='powder'
becomes an IDENTITY grain with substance='self.powder'.

NOTE: I discovered that tokens flow around such that DEDENTS arise not after the last line of indentation but before before the frist dedented line...
Subtle, upshot is in-line comments don;t always turn up in the token stream as one might expect. 
To counter this I have added the 'suspension/bubble-up' concept during purification so that in-line semantics more reliably associate with the correct lineage.
granulator.GRANULATOR:FIGURATION;
Takes an unruly, heterogeneous input bulk material and turns it into an intermediate purified powder which is then refined into grains.

Uses the SAMPLE test bed to inspect each particle and applies track&trace (via the registrar) to give just the particles of interest along with their full lineage.

Then refines particles into grains
granulator.GRANULATOR.__init__.bx_id:KNOWLEDGE;
identity of the overall package of materials
granulator.GRANULATOR.__init__.self:KNOWLEDGE;
resulting list of refined grains as (lineage, type, substance) tuples
granulator.GRANULATOR.dump_powder:MECHANISM;
Shows the powderised bulk raw material
granulator.GRANULATOR.dump_purified:MECHANISM;
Shows the purified bulk raw material powder
granulator.GRANULATOR.dump_bx_record:MECHANISM;
Shows the result of purification
granulator.GRANULATOR.dump_inventory:MECHANISM;
Shows the final set of grains
granulator.GRANULATOR.granulate:BEHAVIOUR;
Assay's the material and performs the granulation
This is a distinct step from initiating the Granulator in case there are any startup issues
(Pythonic mantra: __init__ must succeed)
granulator.GRANULATOR.purify:PROSE;
Interlude in a poet's voice...
=====================================
CANTO I: In which The Sludge is Sloughed
CANTO II: In which The Sludge is Sought
CANTO III: In which The Pure is Preserved
granulator.GRANULATOR.purify:PROSE;
Interlude in a poet's voice...
=====================================
CANTO I: In which The Sludge is Sloughed
CANTO II: In which The Sludge is Sought
CANTO III: In which The Pure is Preserved
granulator.GRANULATOR.fine_mix:PROSE;
On the fine mix process...
--------------------------
Break up suspensions so the DENTs don't come between TEXTs and NAMEs
Evapourate the DENTs so the remainder can be classified
No longer just a powder, the intermediate is ready to be refined into grains
granulator.GRANULATOR.fine_mix:PROSE;
On the fine mix process...
--------------------------
Break up suspensions so the DENTs don't come between TEXTs and NAMEs
Evapourate the DENTs so the remainder can be classified
No longer just a powder, the intermediate is ready to be refined into grains
granulator.GRANULATOR.refine:PROSE;
On the distillation process
=================================================
If we are not already distilling, see if we should
If we are distilling, condense into previous grain
Otherwise create a new grain
granulator.GRANULATOR.refine:PROSE;
On the distillation process
=================================================
If we are not already distilling, see if we should
If we are distilling, condense into previous grain
Otherwise create a new grain
granulator.SAMPLE:AFFORDANCE;
A test bench, or lab, that inspects samples of powder

Casts the symbolic LEXICON of the base CODEX into particle parlance, allowing us to identify how we purify the particles
granulator.SAMPLE.SIEVE:KNOWLEDGE;
Allows impurities to be sieved from the powder - all particles of these objective types are collected by the sieve
granulator.SAMPLE.SLUDGE:KNOWLEDGE;
when to start de-sludging the powder
granulator.SAMPLE.DESLUDGE:KNOWLEDGE;
when we know the powder has been de-sludged
granulator.SAMPLE.FILTERED:KNOWLEDGE;
The particle types that have finescale filters
granulator.SAMPLE.FILTER:KNOWLEDGE;
And The finescale filters they have
granulator.SAMPLE.SUSPENSIONS:KNOWLEDGE;
Particles in suspension flow outside of the norm, we need to bubble some others up when we meet a suspension
granulator.SAMPLE.assay:MECHANISM;
creates the powder from the bulk material
granulator.SAMPLE.sieved:SKILL;
Blocks powder particles that don't fall through the sieve for collection
granulator.SAMPLE.has_sludged:DISPOSITION;
Detects the emergence of a clump of sludge
granulator.SAMPLE.has_desludged:DISPOSITION;
Detects the sludge has been cleared
granulator.SAMPLE.is_filtrate:SKILL;
Applies a finescale filter to otherwise clean sieved powder
granulator.SAMPLE.particle_name:MECHANISM;
AND ultimately, we DO need to know the actual name of a particle!
granulator.SAMPLE.particle_location:MECHANISM;
AND ultimately, we DO need to know the actual name of a particle!
granulator.SAMPLE._is_suspension:MECHANISM;
Tests to see if this particle sits in suspension, and if we ought to bubble the next particle up
granulator.Precursor:KNOWLEDGE;
Classified particles ready to be refined
granulator.GrainType:KNOWLEDGE;
The kinds of grains we make
granulator.Grain:KNOWLEDGE;
just what a grain looks like
granulator.Grain.semantics:MECHANISM;
Allows a grain to be viewed as a semantic entity
granulator.REFINE:AFFORDANCE;
Casts the symbolic LEXICON of the base CODEX into grain parlance, allowing us to identify how we refine the particles
granulator.REFINE.DISTILLANT:KNOWLEDGE;
What kind of particles causes distillation
granulator.REFINE.IDENTITY_GRAINS:KNOWLEDGE;
What kind of particles create IDENTITY grains
granulator.REFINE.TEXT_GRAINS:KNOWLEDGE;
What kind of particles create TEXT grains
granulator.REFINE.is_distillant:DISPOSITION;
Detects when we should distil
granulator.REFINE.get_grain_type:SKILL;
Categorizes particles
lexicographics.dataclasses:CONTINUUM;
allows us to create named structures for attestations, etymologies, and lexemes
lexicographics.HASH_SIGIL:KNOWLEDGE;
If we want to use prose blocks, we need to ensure not to include literal hash in any strings inside the prose block!
lexicographics.ExpoTags:KNOWLEDGE;
The types of semantic meaning we can use to adorn our code-base.
lexicographics.ExpoTags.CONTINUUM:PROSE;
CONTINUUM - alien facets that we use, typically within their own metaphor
THROUGHLINE - the metaphoric interface, explaining the relationship between the module-metaphore and the world at large
FIGURATIONs and AFFORDANCEs - high-order CHARACTERISATIONS providing a semantic package. I'm as yet somewhat unclear on their precise differentiation...
KNOWLEDGE - Typically important datum or data classes
BEHAVIOUR - a small package of sequenced actions
MECHANISM - an action, e.g. getters/setters
SKILL - an ability, e.g. inspect entity, filter list
DISPOSITION - an indication (or detetcion), of state (or transition)
PROSE - story woven around code sections
FLAW - An exception or sentinel
lexicographics:THROUGHLINE;
Earlier processing has delivered parcels consisting of a blend of IDENTITY and TEXT grains

The IDENTITY grains are atomic entities providing the name, Pythonic scope and source references of discovered Python objects in the source code. Obvs these discovered objects can occur in multiple places (since objects are declared so that they can be used). Each occurence is accumulated into an Etymology, and each Etymmology is accumulated into the overall list of all_attestations (an attestation being a recorded evidence of a lexical entity). This results in a list of everything that has been (or can be) refferred to.

The TEXT grains are a muddled collection of discovered strings and comments, so the LEXICOGRAPHER sifts through these looking for those that have semantic meaning, i.e. begin with expositional tags (ExpoTags). When a meaning is disccovered it is (typically) associated with the next attestation of a lexical - i.e semantic meaning becomes attached to the canonical attestation of a lexical. These entities (lexemes) are then accumulated into the all_expositions list so that the semantics of a lexical are known at any point the lexical is found (and in fact, also, where exactly its canonical form can be found).

Mostly, semantics are attached to classes and methods, or to other Pythonic objects (vars) - immediately preceeding that which they define. Special attention has been made to decorators which also must preceded that which they decorate. Semantic expositions are aware of that Python restriction, and have allowed for the intrusion of such (in the earlier processing). There are some exceptions to this semantic binding, and in fact more work needed to make the semantic binding complete, vis-à-vis:
    - THROUGHLINES, are bound to the module; but currently only by convention of their placement in a module
    - CONTINUUM, ought to bind to the last identity in a '[from x] import y [as z]' expression; currently it doesnt and is poorly bound
    - PROSE, is bound to the container (method, class or module) in which it is found, rather than the next specific object, method or class name

Most of the expositions are wholly provisioned by the substance of their text grain - either because they were very simple (found in an in-line comment); or, they were encapsulated within a multiline string within the code.

PROSE expositions are somewhat different. By design the PROSE is woven in and amongst the code using in-line commentary; so that, when we have significant code-blocks, we can annotate it with its story. A '# PROSE:' comment causes all in-line comments to be accumulated, until such time another exposition is encountered. This is convenient, but a little blunt since any normal code comments will also get swept up into the expositionary prose. It's easy to imagine many better methods, but this will do for now!

Oh! Also, any strings that contain '#' will collect garbage into the prose block also! So it really is weak at the moment!!!
lexicographics.LEXICOGRAPHER:FIGURATION;
Sifts through a given set of 'entries' to generate:
- a lexicon of known lexicals(!), erm, I mean a list of things that can be known about
- the linguistical set of those things that have meaning
lexicographics.LEXICOGRAPHER.list_expositions:BEHAVIOUR;
returns a list of lexeme summaries from a linguistical set
lexicographics.LEXICOGRAPHER.print_expositions:BEHAVIOUR;
prints a linguistical set
lexicographics.LEXICOGRAPHER.print_expo:BEHAVIOUR;
formats and prints a single lexeme, indenting as per the etymological depth.
lexicographics.LEXICOGRAPHER._indent:MECHANISM;
performs an identation of a semantic unit
lexicographics.LEXICOGRAPHER.extract:PROSE;
On the extraction of meaning...
-------------------------------
Every entry has some kind of meaning, for meaning is a layered construct
when the meaning relates to one of our lexemes, we're gonna need to find the following lexical (probably)
When the meaning we found is a lexical's name, create or extend the etymology with this attestation
Otherwise unpack this meaning to extract any semantics it contains relevant to our lexemes
clean-up the extracted semantics...
lexicographics.LEXICOGRAPHER.extract:PROSE;
On the extraction of meaning...
-------------------------------
Every entry has some kind of meaning, for meaning is a layered construct
when the meaning relates to one of our lexemes, we're gonna need to find the following lexical (probably)
When the meaning we found is a lexical's name, create or extend the etymology with this attestation
Otherwise unpack this meaning to extract any semantics it contains relevant to our lexemes
clean-up the extracted semantics...
lexicographics.LEXICOGRAPHER._unpack_text_entry:BEHAVIOUR;
Finds the lexical to associate with a semenatic TEXT, dropping TEXTs that are deifnitely NOT semantic.
Keeps all COMMENT type texts as they are handled later when we package up any PROSE
lexicographics.LEXICOGRAPHER._nonjudgemental_clean:PROSE;
On cleaning the TEXTs...
------------------------
Firstly we deal with COMMENT type texts
If they are in-line semantics (except PROSE) we return them without the comment marker
otherwise in-line comments are returned unadulterated, so the prose block handler has themm available later.

Then we remove any text delimiters around the semantic content.
Being careful only to consider delimiters, not any old quote-mark that might be within the semantic text
somehow we got a string that isn't delimited, weird but okay
somehow the triple quoted string hasn't been terminated, so don't clip the right side
somehow the single quoted string hasn't been terminated, so don't clip the right side
Because the work is a little complex, we have a catch-all return of the unadulterated text - just in case someone decides to add a bug in the code laters...
lexicographics.LEXICOGRAPHER._nonjudgemental_clean:PROSE;
On cleaning the TEXTs...
------------------------
Firstly we deal with COMMENT type texts
If they are in-line semantics (except PROSE) we return them without the comment marker
otherwise in-line comments are returned unadulterated, so the prose block handler has themm available later.

Then we remove any text delimiters around the semantic content.
Being careful only to consider delimiters, not any old quote-mark that might be within the semantic text
somehow we got a string that isn't delimited, weird but okay
somehow the triple quoted string hasn't been terminated, so don't clip the right side
somehow the single quoted string hasn't been terminated, so don't clip the right side
Because the work is a little complex, we have a catch-all return of the unadulterated text - just in case someone decides to add a bug in the code laters...
lexicographics.LEXICOGRAPHER._is_expo:SKILL;
Determines if a text IS semantic
lexicographics.LEXICOGRAPHER.package_prose:PROSE;
Some prose on packaging prose...
---------------------------------
At this point the TEXTs are still a little muddled, you know how strings like to tie themselves into knots right?
Although we removed TEXTs that are not tagged as exposition, we elected to keep all in-line comments so we can block-up interwoven prose...
...so BEWARE we might have rogue strings that happened to start with the in-line comment marker!
At least we now know that any TEXT that doesn't start with HASH, IS a true semantic exposition, so we can focus on the HASH lines here

We will either keep, drop or merge the HASH lines - so we will end up with fewer TEXTs; lets start with an empty list that will hold the survivors
and and empty package into which we build-up the texts to be merged.

Now looking at each text, we initially have no impetus to merge them together...
We will start merging if this is an in-line comment that introduces PROSE
(and note how I avoid creating a string that LOOKS like an expositional tag, awkward I admit)
And we stop merging when we hit another exposition (that is, a line that doesn't start with HASH)

While we are merging we pour the lines into our package, without the comment marker which is now obviated, redundant, utterly useless to us.

If we are not merging, we add any previous merged package to the survivor's list...
...and we add this text to the survivors list, unless its just some itinerant programmer's comment (outside of a prose block)

AND... a final flush if prose block reaches EOF
lexicographics.LEXICOGRAPHER.package_prose:PROSE;
Some prose on packaging prose...
---------------------------------
At this point the TEXTs are still a little muddled, you know how strings like to tie themselves into knots right?
Although we removed TEXTs that are not tagged as exposition, we elected to keep all in-line comments so we can block-up interwoven prose...
...so BEWARE we might have rogue strings that happened to start with the in-line comment marker!
At least we now know that any TEXT that doesn't start with HASH, IS a true semantic exposition, so we can focus on the HASH lines here

We will either keep, drop or merge the HASH lines - so we will end up with fewer TEXTs; lets start with an empty list that will hold the survivors
and and empty package into which we build-up the texts to be merged.

Now looking at each text, we initially have no impetus to merge them together...
We will start merging if this is an in-line comment that introduces PROSE
(and note how I avoid creating a string that LOOKS like an expositional tag, awkward I admit)
And we stop merging when we hit another exposition (that is, a line that doesn't start with HASH)

While we are merging we pour the lines into our package, without the comment marker which is now obviated, redundant, utterly useless to us.

If we are not merging, we add any previous merged package to the survivor's list...
...and we add this text to the survivors list, unless its just some itinerant programmer's comment (outside of a prose block)

AND... a final flush if prose block reaches EOF
lexicographics.LexicalOccurence:KNOWLEDGE;
holds an attestation contextualised lexical entity
lexicographics.Lexeme:KNOWLEDGE;
holds a lexeme - the canonical occurence, category and semantic content of a lexical
lexicographics.Lexeme.from_parts:MECHANISM;
Creates a lexeme by extracting category from a semantic text
lexicographics.Lexeme.summary:SKILL;
Summarises a lexeme to category and canonical reference
lexicographics.Lexeme._dedent:MECHANISM;
The discovered semantics are indented, partly due to the requirements of the originating code, and partly for semantic clarity.
Here we remove the common margin (minimum indent) found within the semantic content.
lexicographics.Etymology:KNOWLEDGE;
holds a list of all occurances of a certain lexical (i.e. with an attestation), noting also where the canonical reference can be found
registrar:THROUGHLINE;
Every token of interest from the parse (i.e. those that survived the granulator's purification stage) visits the Registrar — which may seem draconian, but such is the nature of symbolic governance.

The Registrar’s records are private. The only sanctioned access is through `record_history`, which returns the current known lineage for each notable recorded subject.

Most subjects are notable and have their lineage recorded.
Though draconian, the process remains relatively democratic.

Unnotables are the DENTS.
The Registrar must still track them, as a DEDENT may signal the end of a lineage — depending on the INDENTS that preceded it.

Lineage only extends when a true identity is married to a progenitor (`class` or `def` + name).
The resilience of this family line then waxes and wanes, governed by INDENTS and DEDENTS.

When resilience falls back to the level of the originating progenitor, the family line dies out, and the current lineage contracts.

Lineage is NOT returned for growth/decline subjects (DENTS), as they are not actual 'things' — just indicators of resilience.

Similarly, lineage is NOT returned for honourifics, since they merely address things, but are not things themselves.
registrar.REGISTRAR:FIGURATION;
The Registrar recognises and declares subject titles according to the evolving lineage of recorded subjects.

Some subjects (DENTS and PROGENITORS) do not receive titles, but they influence the shape and continuity of the lineage.

Other subjects (HONOURIFICS) are not recorded at all — they serve only to address true subjects, and are thus excluded from lineage.
registrar.REGISTRAR.__init__.self:KNOWLEDGE;
the true identity of an heir apparent
registrar.REGISTRAR.record_history:PROSE;
on how the code-tree grows and withers as we meet
progenitors, identities and closures
=================================================
First, keep an eye on the resilience of the current family line
------------------------------------------------------------
Now is the time to add any found heir to the lineage
So that GOING FORWARD the title is recognised
----------------------------------------------------
Once we find a new heir, keep it safe for now
So the title is awarded on the next cycle
Otherwise we lose sight of this heir's own lineage
------------------------------------------------------
Have a look-see if we have met a new heir-apparent
--------------------------------------------------
Ultimately branches of a tree die out, and we have to
prune back to continue the ancestral lines
-----------------------------------------------------
Finally all are remembered in the trace of lineage they leave behind 
BUT it is only the true that get entitled
--------------------------------------------------------------------
registrar.REGISTRAR.record_history:PROSE;
on how the code-tree grows and withers as we meet
progenitors, identities and closures
=================================================
First, keep an eye on the resilience of the current family line
------------------------------------------------------------
Now is the time to add any found heir to the lineage
So that GOING FORWARD the title is recognised
----------------------------------------------------
Once we find a new heir, keep it safe for now
So the title is awarded on the next cycle
Otherwise we lose sight of this heir's own lineage
------------------------------------------------------
Have a look-see if we have met a new heir-apparent
--------------------------------------------------
Ultimately branches of a tree die out, and we have to
prune back to continue the ancestral lines
-----------------------------------------------------
Finally all are remembered in the trace of lineage they leave behind 
BUT it is only the true that get entitled
--------------------------------------------------------------------
registrar.REGISTRAR._entitle:SKILL;
Joins up all the identities in our current lineage to form a single, recordable, title
registrar.REGISTRAR._new_record:SKILL;
Prepares a progenitor's new lineage, in case there is then a marriage
registrar.REGISTRAR._fill_in_record:SKILL;
Fills in the new lineage, and sets the baseline of this family line's resilience,
which builds upon that of previous generations
registrar.REGISTRAR._sign_off_record:SKILL;
Marks the end of a family line by removing its lineage
registrar.REGISTRAR._register_empty:FLAW;
Allows us to check if the register is empty before we try to remove a family line
registrar.LINEAGE:AFFORDANCE;
Casts the symbolic LEXICON of the base CODEX into lineage parlance, allowing us to identify lineage related ENTITIES
registrar.LINEAGE.PROGENITORS:KNOWLEDGE;
The subjects that (potentially) start a new generation in the lineage
registrar.LINEAGE.HONOURIFICS:KNOWLEDGE;
The ways in which subjects may be addressed, not actual subject identities
registrar.LINEAGE.IDENTITIES:KNOWLEDGE;
The subjects that may be true identities
registrar.LINEAGE.GROWTH:KNOWLEDGE;
Represents a waxing in the current family-line's resilience
registrar.LINEAGE.DECLINE:KNOWLEDGE;
Represents a waning in the current family-line's resilience
registrar.LINEAGE.TRUE_SUBJECTS:KNOWLEDGE;
True subjects - the things we want to record
registrar.LINEAGE.is_true_subject:SKILL;
Matches progenitor type subjects only
registrar.LINEAGE.is_progenitor:SKILL;
Matches progenitor type subjects only
registrar.LINEAGE.is_true_identity:SKILL;
Matches subjects that are true identities (not honourifics)
registrar.LINEAGE.growth:SKILL;
Detects when the current family line grows
registrar.LINEAGE.decline:SKILL;
Detects when a current family line has declined
registrar.LINEAGE.subject_name:MECHANISM;
AND ultimately, we DO need to know the actual name of a subject!
