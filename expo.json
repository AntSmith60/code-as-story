{
  "codices.token": {
    "category": "CONTINUUM",
    "canonical": "codices.token",
    "content": "Lets us know the structure of a token, specifically, type and value (.string)",
    "reference": "(1, 0)"
  },
  "codices.tokenize": {
    "category": "CONTINUUM",
    "canonical": "codices.tokenize",
    "content": "Tokenizes a text stream in-line with Python  syntax",
    "reference": "(3, 0)"
  },
  "codices": {
    "category": "THROUGHLINE",
    "canonical": "codices",
    "content": "Behind all the metaphor, we are parsing Python script files - using the standard `tokenize` library to extract symbolic structure from source code.\n\n\n\nTokenization gives us a symbolic view of the source, simplifying parsing.\n\nIt turns complex syntax into a simple (though intermediate) lexicon.\n\n\n\nSo:\n\n>def parse(source):\n\n\n\nbecomes:\n\n>NAME NAME OP NAME OP OP\n\n\n\nTokens have limited understanding of their role, but great clarity about what they are.\n\nFor example, a token knows that 'def' is a NAME, but not that it's a Python reserved word.\n\n\n\nMeaning emerges through layered symbolism.\n\n\n\nSo:\n\n>NAME NAME OP NAME OP OP\n\n\n\nmight, if parsing for execution, become:\n\n>keyword identity lbrace identity rbrace colon\n\n\n\nBut we are not parsing to execute \u2014 we are parsing to extract a limited set of precise entities:\n\nprescriptive texts and the things they prescribe, within their Pythonic scope.\n\n\n\nWe do not care about things like braces and colons.\n\n\n\nThe base CODEX provides the layered symbolism we do care about.\n\nFor example, it tells us whether a NAME token is an ENCAPSULATOR \u2014\n\na symbol that opens a new scope (like `def` or `class`).\n\n\n\nNote: this module fully wraps the TOKEN structure, which is never directly referenced elsewhere.",
    "reference": "(6, 0)"
  },
  "codices.CODEX_OBJECTS": {
    "category": "KNOWLEDGE",
    "canonical": "codices.CODEX_OBJECTS",
    "content": "Objects define the nature of subjects. E.g. the subject 'def' is a NAME object, in raw token parlance.",
    "reference": "(42, 0)"
  },
  "codices.CODEX": {
    "category": "AFFORDANCE",
    "canonical": "codices.CODEX",
    "content": "Though literally a tree trunk (from the Latin *codex*), the term has come to signify a book of law \u2014 or more precisely, of lore.\n\n\n\nNot 'law' in the contemporary sense, but rather 'ritual' or 'rite': the correct sequence of symbols that achieves a result.\n\n\n\nAnd herein, our codices do just that \u2014 they prescribe the correct sequence of things in terms of their symbolism (ruinic nature).\n\n\n\nIn the base CODEX, very little is known beyond the symbolic meaning of what occurs in the provided sequence.\n\n\n\nThus, the base CODEX offers:\n\n- a symbolic lexicon (as a dict),\n\n- a means to generate the sequence (objectify the source),\n\n- and a way to discern the true name (or value) of encountered symbols.",
    "reference": "(46, 0)"
  },
  "codices.CODEX.LEXICON": {
    "category": "KNOWLEDGE",
    "canonical": "codices.CODEX.LEXICON",
    "content": "Symbolises specific python token subjects that direct our layered parsing",
    "reference": "(62, 4)"
  },
  "codices.CODEX.objectify": {
    "category": "MECHANISM",
    "canonical": "codices.CODEX.objectify",
    "content": "Encapsulates the tokenisation process",
    "reference": "(87, 4)"
  },
  "codices.CODEX.token_val": {
    "category": "MECHANISM",
    "canonical": "codices.CODEX.token_val",
    "content": "We want to contain ALL token structure knowledge to the base CODEX\n\nso here's how we get to what a token actually IS.",
    "reference": "(95, 4)"
  },
  "codices.CODEX.token_start": {
    "category": "MECHANISM",
    "canonical": "codices.CODEX.token_start",
    "content": "We want to contain ALL token structure knowledge to the base CODEX\n\nso here's how we get where the token occurred",
    "reference": "(103, 4)"
  },
  "codices.ENTITY": {
    "category": "AFFORDANCE",
    "canonical": "codices.ENTITY",
    "content": "Derived codices apply the base CODEX LEXICON through sets of ENTITIES \u2014 statements of what we care about, expressed as layered symbolism, in the derived metaphor.\n\n\n\nFor example, when building a lineage, we want to ignore Python RESERVED words.\n\nWe're interested only in NAME tokens that represent actual Pythonic objects (variables, classes, etc).\n\n\n\nReserved words like `def` and `yield` serve as HONOURIFICS\u2014titles that address true identities.\n\n\n\nSo we might define:\n\n    HONOURIFICS = ENTITY('NAME', 'RESERVED')\n\nE.g. an HONOURIFIC as any NAME in the set of RESERVED values\n\n\n\nEntities are compound matching structures \u2014 ritual instruments that determine whether a given 'thing' satisfies the lore of the codex.\n\n\n\nEach codex offers a distinct lens on shared material, and begins by defining the entities of interest.\n\nThis class serves as their oracle, revealing when those entities have been fulfilled \u2014 or left wanting.\n\n\n\nThe 'things' we encounter consist of both type and value.\n\nEntities allow us to match by type alone, or by both type and value.",
    "reference": "(112, 0)"
  },
  "codices.ENTITY.add": {
    "category": "MECHANISM",
    "canonical": "codices.ENTITY.add",
    "content": "Extends the ENTITY's potence by adding more types and values it recognises.",
    "reference": "(139, 4)"
  },
  "codices.ENTITY.is_entity": {
    "category": "SKILL",
    "canonical": "codices.ENTITY.is_entity",
    "content": "Determines if this ENTITY recognise a given token.",
    "reference": "(150, 4)"
  },
  "granulator.os": {
    "category": "CONTINUUM",
    "canonical": "granulator.os",
    "content": "allows conversion of an underlying filepath to a batch identity",
    "reference": "(1, 0)"
  },
  "granulator.dataclasses": {
    "category": "CONTINUUM",
    "canonical": "granulator.dataclasses",
    "content": "allows us to create a named structure for the final substance list we produce",
    "reference": "(4, 0)"
  },
  "granulator": {
    "category": "THROUGHLINE",
    "canonical": "granulator",
    "content": "We pour a batch of Python source code (.py file) into the Granulator.\n\n\n\nThrough a series of processing steps, we generate an inventory of 'grains'\u2014each providing:\n\n- lineage: where, in the original bulk material, this grain was found\n\n- type: whether it is a TEXT description or an object IDENTITY\n\n- substance: the found TEXT or IDENTITY itself\n\n- location: in the original source material\n\n- progenitor: is it the first of a new line?\n\n\n\nMetaphorically, the Python source code is the bulk_material we work on.\n\nIt is tokenized into a powder of token particles, which are then purified and mixed into a precursor for refinement into grains.\n\n\n\nParticles are tracked through a batch record, capturing the Pythonic scope in which each particle is found.\n\n\n\nParticles are distilled into grains such that a sequence like:\n\n>NAME.string='self' OP.string='.' NAME.string='powder'\n\n\n\nbecomes an IDENTITY grain with substance='self.powder'.\n\n\n\nNOTE: I discovered that tokens flow around such that DEDENTS arise not after the last line of indentation but immediately before the first dedented line...\n\nSubtle, upshot is in-line comments don't always turn up in the token stream as one might expect. \n\nTo counter this I have added the 'suspension/bubble-up' concept during the fine-mix so that in-line semantics more reliably associate with the correct lineage.",
    "reference": "(14, 0)"
  },
  "granulator.GRANULATOR": {
    "category": "FIGURATION",
    "canonical": "granulator.GRANULATOR",
    "content": "Takes an unruly, heterogeneous input bulk material and turns it into an intermediate purified precursor which is then refined into grains.\n\n\n\nUses the SAMPLE test bed to inspect each particle and applies track&trace (via the registrar) to give just the particles of interest along with their full lineage.\n\n\n\nThen refines particles into grains",
    "reference": "(40, 0)"
  },
  "granulator.GRANULATOR.__init__.bx_id": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GRANULATOR.__init__.bx_id",
    "content": "identity of the overall package of materials",
    "reference": "(50, 8)"
  },
  "granulator.GRANULATOR.__init__.self.powder": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GRANULATOR.__init__.self.powder",
    "content": "Full catalogue of the original material, as particles",
    "reference": "(57, 8)"
  },
  "granulator.GRANULATOR.__init__.self.purified": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GRANULATOR.__init__.self.purified",
    "content": "list of purified particles",
    "reference": "(60, 8)"
  },
  "granulator.GRANULATOR.__init__.self.intermediate": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GRANULATOR.__init__.self.intermediate",
    "content": "list of intermediates as Precursor class",
    "reference": "(63, 8)"
  },
  "granulator.GRANULATOR.__init__.self.refined": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GRANULATOR.__init__.self.refined",
    "content": "resulting list of refined grains as Grain class",
    "reference": "(66, 8)"
  },
  "granulator.GRANULATOR.dump_powder": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR.dump_powder",
    "content": "Shows the powderised bulk raw material",
    "reference": "(69, 4)"
  },
  "granulator.GRANULATOR.dump_purified": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR.dump_purified",
    "content": "Shows the purified bulk raw material powder",
    "reference": "(76, 4)"
  },
  "granulator.GRANULATOR.dump_bx_record": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR.dump_bx_record",
    "content": "Shows the tracked intermediate precursors",
    "reference": "(86, 4)"
  },
  "granulator.GRANULATOR.dump_inventory": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR.dump_inventory",
    "content": "Shows the final set of grains",
    "reference": "(93, 4)"
  },
  "granulator.GRANULATOR.granulate": {
    "category": "BEHAVIOUR",
    "canonical": "granulator.GRANULATOR.granulate",
    "content": "Assay's the material and performs the granulation. \n\nThis is a distinct step from initiating the Granulator in case there are any startup issues\n\n(Pythonic mantra: __init__ must succeed)",
    "reference": "(114, 4)"
  },
  "granulator.GRANULATOR.purify": {
    "category": "BEHAVIOUR",
    "canonical": "granulator.GRANULATOR.purify",
    "content": "Purifies the powder by sieving for particles of interest\n\nALSO passes the sieved powder through a fine-scale filter\n\nAND detects/discards any clumpy sludge found in the powder\n\n\n\nEach purified particle is classified from the on-going batch records\n\n Interlude in a poet's voice...\n\n- CANTO I: In which The Sludge is Sloughed\n- if is sludge and has not desludged\n- CANTO II: In which The Sludge is Sought\n- if not is sludge and now has sludged\n- CANTO III: In which The Pure is Preserved\n- if you can sieve and be filtrate then \n- - you'll be Purified, my son!",
    "reference": "(135, 4)"
  },
  "granulator.GRANULATOR.fine_mix": {
    "category": "BEHAVIOUR",
    "canonical": "granulator.GRANULATOR.fine_mix",
    "content": "Sometimes particles form a suspension (COMMENTS + DENTS) that needs breaking up so that the particles will bond correctly when we refine them into grains.\n\nMixing allows the DENTs to bubble up so they evapourate as we inspect and classify the remainder\n\n On the fine mix process...\n\n- Break up suspensions so the DENTs don't come between TEXTs and NAMEs\n- Evapourate the DENTs so the remainder can be classified\n- No longer just a powder, the intermediate is ready to be refined into grains",
    "reference": "(173, 4)"
  },
  "granulator.GRANULATOR._mix": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR._mix",
    "content": "Mixes the purified powder, breaking up suspensions that effect how particles adhere into grains during refinement",
    "reference": "(190, 4)"
  },
  "granulator.GRANULATOR._evapourate": {
    "category": "MECHANISM",
    "canonical": "granulator.GRANULATOR._evapourate",
    "content": "Applies track&trace while condensing the intermediate to just the components that will make up the refined IDENTITY and TEXT grains.",
    "reference": "(205, 4)"
  },
  "granulator.GRANULATOR.refine": {
    "category": "BEHAVIOUR",
    "canonical": "granulator.GRANULATOR.refine",
    "content": "From purification we now have muddled comments and strings which will become the TEXTS\n\n  and split Identities with sequences of NAME(.NAME...)s\n\n  Refines NAMEs through distillation\n\n On the distillation process\n\n- If we are not already distilling, see if we should\n- If we are distilling, condense into previous grain\n- Otherwise create a new grain",
    "reference": "(225, 4)"
  },
  "granulator.SAMPLE": {
    "category": "AFFORDANCE",
    "canonical": "granulator.SAMPLE",
    "content": "A test bench, or lab, that inspects samples of powder\n\n\n\nCasts the symbolic LEXICON of the base CODEX into particle parlance, allowing us to identify how we purify the particles",
    "reference": "(267, 0)"
  },
  "granulator.SAMPLE.SIEVE": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.SIEVE",
    "content": "Allows impurities to be sieved from the powder - all particles of these objective types are collected by the sieve",
    "reference": "(274, 4)"
  },
  "granulator.SAMPLE.SLUDGE": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.SLUDGE",
    "content": "when to start de-sludging the powder",
    "reference": "(282, 4)"
  },
  "granulator.SAMPLE.DESLUDGE": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.DESLUDGE",
    "content": "when we know the powder has been de-sludged",
    "reference": "(285, 4)"
  },
  "granulator.SAMPLE.FILTERED": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.FILTERED",
    "content": "The particle types that have finescale filters",
    "reference": "(288, 4)"
  },
  "granulator.SAMPLE.FILTER": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.FILTER",
    "content": "And The finescale filters they have",
    "reference": "(291, 4)"
  },
  "granulator.SAMPLE.SUSPENSIONS": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.SAMPLE.SUSPENSIONS",
    "content": "Particles in suspension flow outside of the norm, we need to bubble some others up when we meet a suspension",
    "reference": "(294, 4)"
  },
  "granulator.SAMPLE.assay": {
    "category": "MECHANISM",
    "canonical": "granulator.SAMPLE.assay",
    "content": "creates the powder from the bulk material",
    "reference": "(299, 4)"
  },
  "granulator.SAMPLE.sieved": {
    "category": "SKILL",
    "canonical": "granulator.SAMPLE.sieved",
    "content": "Blocks powder particles that don't fall through the sieve for collection",
    "reference": "(307, 4)"
  },
  "granulator.SAMPLE.has_sludged": {
    "category": "DISPOSITION",
    "canonical": "granulator.SAMPLE.has_sludged",
    "content": "Detects the emergence of a clump of sludge",
    "reference": "(315, 4)"
  },
  "granulator.SAMPLE.has_desludged": {
    "category": "DISPOSITION",
    "canonical": "granulator.SAMPLE.has_desludged",
    "content": "Detects the sludge has been cleared",
    "reference": "(323, 4)"
  },
  "granulator.SAMPLE.is_filtrate": {
    "category": "SKILL",
    "canonical": "granulator.SAMPLE.is_filtrate",
    "content": "Applies a finescale filter to otherwise clean sieved powder",
    "reference": "(331, 4)"
  },
  "granulator.SAMPLE.particle_name": {
    "category": "MECHANISM",
    "canonical": "granulator.SAMPLE.particle_name",
    "content": "we DO need to know the actual name of a particle!",
    "reference": "(344, 4)"
  },
  "granulator.SAMPLE.particle_location": {
    "category": "MECHANISM",
    "canonical": "granulator.SAMPLE.particle_location",
    "content": "A precise reference that ties this particle concept back to the originating source code",
    "reference": "(352, 4)"
  },
  "granulator.SAMPLE._is_suspension": {
    "category": "MECHANISM",
    "canonical": "granulator.SAMPLE._is_suspension",
    "content": "Tests to see if this particle sits in suspension, and if we ought to bubble the next particle up",
    "reference": "(361, 4)"
  },
  "granulator.Precursor": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.Precursor",
    "content": "Classified particles ready to be refined (particles + classification)",
    "reference": "(373, 0)"
  },
  "granulator.GrainType": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.GrainType",
    "content": "The kinds of grains we make, i.e. the basis for discerning lexical identity and semantic meaning",
    "reference": "(402, 0)"
  },
  "granulator.Grain": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.Grain",
    "content": "just what a grain looks like - i.e. lineage, type, content, canonicalism and original bulk material reference",
    "reference": "(408, 0)"
  },
  "granulator.Grain.semantics": {
    "category": "MECHANISM",
    "canonical": "granulator.Grain.semantics",
    "content": "Allows a grain to be viewed as a semantic entity",
    "reference": "(417, 4)"
  },
  "granulator.REFINE": {
    "category": "AFFORDANCE",
    "canonical": "granulator.REFINE",
    "content": "Casts the symbolic LEXICON of the base CODEX into grain parlance, allowing us to identify how we refine the particles",
    "reference": "(454, 0)"
  },
  "granulator.REFINE.DISTILLANT": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.REFINE.DISTILLANT",
    "content": "What kind of particles causes distillation",
    "reference": "(459, 4)"
  },
  "granulator.REFINE.IDENTITY_GRAINS": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.REFINE.IDENTITY_GRAINS",
    "content": "What kind of particles create IDENTITY grains",
    "reference": "(462, 4)"
  },
  "granulator.REFINE.TEXT_GRAINS": {
    "category": "KNOWLEDGE",
    "canonical": "granulator.REFINE.TEXT_GRAINS",
    "content": "What kind of particles create TEXT grains",
    "reference": "(465, 4)"
  },
  "granulator.REFINE.is_distillant": {
    "category": "DISPOSITION",
    "canonical": "granulator.REFINE.is_distillant",
    "content": "Detects when we should distil",
    "reference": "(469, 4)"
  },
  "granulator.REFINE.get_grain_type": {
    "category": "SKILL",
    "canonical": "granulator.REFINE.get_grain_type",
    "content": "Categorizes particles",
    "reference": "(477, 4)"
  },
  "lexicographer.json": {
    "category": "CONTINUUM",
    "canonical": "lexicographer.json",
    "content": "allows us to format and export our linguistic set as JSON",
    "reference": "(1, 0)"
  },
  "lexicographer.os": {
    "category": "CONTINUUM",
    "canonical": "lexicographer.os",
    "content": "allows us to detect if we are creating or updating the index TXT file",
    "reference": "(5, 0)"
  },
  "lexicographer": {
    "category": "THROUGHLINE",
    "canonical": "lexicographer",
    "content": "Earlier processing has delivered parcels consisting of a blend of IDENTITY and TEXT grains\n\n\n\nThe IDENTITY grains are atomic entities providing the name, Pythonic scope and source references of discovered Python objects in the source code. Obvs these discovered objects can occur in multiple places (since objects are declared so that they can be used), but we only care about existence to help bind semantics to lexicals. The occurences are accumulated into an overall list of attestations (an attestation being a recorded evidence of a lexical entity). This results in a list of everything that has been (or can be) refferred to.\n\n\n\nThe TEXT grains are a muddled collection of discovered strings and comments, so the LEXICOGRAPHER sifts through these looking for those that have semantic meaning, i.e. begin with expositional tags (ExpoTags). When a meaning is disccovered it is (typically) associated with the next attestation of a lexical - i.e semantic meaning becomes attached to the canonical attestation of a lexical. These entities (lexemes) are then accumulated into the all_expositions list so that the semantics of a lexical are known at any point the lexical is found (and in fact, also, where exactly its canonical form can be found).\n\n\n\nMostly, semantics are attached to classes and methods, or to other Pythonic objects (vars) - immediately preceeding that which they define[^1]. There are some exceptions to this semantic binding, and in fact more work needed to make the semantic binding complete, vis-\u00e0-vis:\n\n- THROUGHLINES, are bound to the module; but currently only by convention of their placement in a module\n\n- CONTINUUM, ought to bind to the last identity in a '[from x] import y [as z]' expression; currently it doesnt and is poorly bound\n\n- PROSE, is bound to the most recent semantic entity; i.e. PROSE extends the current semantic with a list of detailed steps\n\n\n\n[^1]:Special attention has been made to decorators which also must preceded that which they decorate. Semantic expositions are aware of that Python restriction, and have allowed for the intrusion of such (in the earlier processing).\n\n\n\nMost of the expositions are wholly provisioned by the substance of their text grain - either because they were very simple (found in an in-line comment); or, they were encapsulated within a multiline string within the code.\n\n\n\nPROSE expositions are somewhat different. By design the PROSE is woven in and amongst the code using in-line commentary; so that, when we have significant code-blocks, we can annotate it with its story. A '# PROSE:' comment causes all in-line comments to be accumulated, until such time another exposition is encountered. This is convenient, but a little blunt since any normal code comments will also get swept up into the expositionary prose. It's easy to imagine many better methods, but this will do for now!",
    "reference": "(12, 0)"
  },
  "lexicographer.LEXICOGRAPHER": {
    "category": "FIGURATION",
    "canonical": "lexicographer.LEXICOGRAPHER",
    "content": "Sifts through a given set of 'entries' to generate:\n\n- a lexicon of known lexicals(!), erm, I mean a list of things that can be known about\n\n- the linguistical set of those things that have meaning",
    "reference": "(32, 0)"
  },
  "lexicographer.LEXICOGRAPHER.save_to_file": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER.save_to_file",
    "content": "Creates a json file containing the full linguistic set and a text file listing the canonicals",
    "reference": "(48, 4)"
  },
  "lexicographer.LEXICOGRAPHER.list_expositions": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER.list_expositions",
    "content": "returns a list of lexeme summaries from a linguistical set",
    "reference": "(81, 4)"
  },
  "lexicographer.LEXICOGRAPHER.print_expositions": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER.print_expositions",
    "content": "prints a linguistical set",
    "reference": "(98, 4)"
  },
  "lexicographer.LEXICOGRAPHER.print_expo": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER.print_expo",
    "content": "formats and prints a single lexeme, indenting as per the depth of its attestation.",
    "reference": "(109, 4)"
  },
  "lexicographer.LEXICOGRAPHER._indent": {
    "category": "MECHANISM",
    "canonical": "lexicographer.LEXICOGRAPHER._indent",
    "content": "performs an identation of a semantic unit as decoration for direct printed output",
    "reference": "(150, 4)"
  },
  "lexicographer.LEXICOGRAPHER.extract": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER.extract",
    "content": "Sifts through the entries for TEXTs to generate the semantics which are combined to lexicals to ppprovide our lexemes\n\n On the extraction of meaning...\n\n- Every entry has some kind of meaning, for meaning is a layered construct\n- when the meaning relates to one of our lexemes, we're gonna need to find the following lexical (probably)\n- At this point we only care about this TEXT's semantic content and the next IDENTITY's lexical value\n- clean-up the extracted semantics...",
    "reference": "(163, 4)"
  },
  "lexicographer.LEXICOGRAPHER._package_prose": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographer.LEXICOGRAPHER._package_prose",
    "content": "Up to now we have preserved semantic PROSE and non-semantic comment-type texts so we can unpick the commentary from the code in order to build prose blocks.\n\nThis is where we collate comment texts within a prose section to form a single semantic prose block - throwing away comment type texts that are NOT inside a prose section.\n\n Some prose on packaging prose...\n\n- At this point the TEXTs are still a little muddled, you know how strings like to tie themselves into knots right?\n- Although we removed TEXTs that are not tagged as exposition, we elected to keep all in-line comments so we can block-up interwoven prose\n- At least we now know that any TEXT that doesn't start with HASH, IS a true semantic exposition, so we can focus on the HASH lines here\n- We will either keep, drop or merge the HASH lines - so we will end up with fewer TEXTs; lets start with an empty list that will hold the survivors\n- and an empty package into which we build-up the texts to be merged.\n- Now looking at each text, we initially have no impetus to merge them together...\n- We will start merging if this is an in-line comment that introduces PROSE\n- While we are merging we pour the lines into our package, without the comment marker which is now obviated, redundant, utterly useless to us.\n- AND... a final flush if prose block reaches EOF",
    "reference": "(194, 4)"
  },
  "lexicographer.LEXICOGRAPHER._update_survivors": {
    "category": "MECHANISM",
    "canonical": "lexicographer.LEXICOGRAPHER._update_survivors",
    "content": "Adds any package of semantics we have been collating to the latest survivor before adding this survivor also\n\nunless this survivor is just  some itinerant programmer's comment (outside of a prose block)",
    "reference": "(239, 4)"
  },
  "lexicographer.LEXICOGRAPHER._update_semantic_package": {
    "category": "MECHANISM",
    "canonical": "lexicographer.LEXICOGRAPHER._update_semantic_package",
    "content": "Adds the relevant parts of the current semantic to the packaged semantic.\n\nI.e. store the reference and lexical for the first packaged text, and the semantic for all packaged texts",
    "reference": "(260, 4)"
  },
  "lexicographics.dataclasses": {
    "category": "CONTINUUM",
    "canonical": "lexicographics.dataclasses",
    "content": "allows us to create named structures for attestations, and lexemes",
    "reference": "(1, 0)"
  },
  "lexicographics.enum": {
    "category": "CONTINUUM",
    "canonical": "lexicographics.enum",
    "content": "allows us to create the ExpoTags (Enum) list",
    "reference": "(6, 0)"
  },
  "lexicographics.ExpoTags": {
    "category": "KNOWLEDGE",
    "canonical": "lexicographics.ExpoTags",
    "content": "The types of semantic meaning we can use to adorn our code-base.\n\n\n\n- WORLD - overarching description, main goal\n- CONTINUUM - alien facets that we use, typically within their own metaphor\n- THROUGHLINE - the metaphoric interface, explaining the relationship between the module-metaphore and the world at large\n- FIGURATIONs and AFFORDANCEs - high-order CHARACTERISATIONS providing a semantic package. I'm as yet somewhat undecided on their precise differentiation...\n- KNOWLEDGE - Typically important datum or data classes\n- BEHAVIOUR - a small package of sequenced actions\n- MECHANISM - an action, e.g. getters/setters\n- SKILL - an ability, e.g. inspect entity, filter list\n- DISPOSITION - an indication (or detetcion), of state (or transition)\n- PROSE - story woven around code sections\n- FLAW - An exception or sentinel",
    "reference": "(11, 0)"
  },
  "lexicographics.LEXICOGRAPHICS": {
    "category": "AFFORDANCE",
    "canonical": "lexicographics.LEXICOGRAPHICS",
    "content": "The core of a lexicographer's abilities",
    "reference": "(45, 0)"
  },
  "lexicographics.LEXICOGRAPHICS.unpack_text_entry": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographics.LEXICOGRAPHICS.unpack_text_entry",
    "content": "Finds the lexical to associate with a semenatic TEXT, dropping TEXTs that are deifnitely NOT semantic.\n\nKeeps all COMMENT type texts as they are handled later when we package up any PROSE",
    "reference": "(50, 4)"
  },
  "lexicographics.LEXICOGRAPHICS._nonjudgemental_clean": {
    "category": "BEHAVIOUR",
    "canonical": "lexicographics.LEXICOGRAPHICS._nonjudgemental_clean",
    "content": "Essentially strips delimiting quotes from a text, but doesn't get all judgy if the text is somehow poorly delimited.\n\nAlso removes commentary markers from in-line semantics (except PROSE which is cleaned up later)\n\n On cleaning the TEXTs...\n\n- Firstly we deal with COMMENT type texts\n- If they are in-line semantics (except PROSE) we return them without the comment marker\n- otherwise in-line comments are returned unadulterated, so the prose block handler has them available later.\n- Then we remove any text delimiters around the semantic content.\n- Being careful only to consider delimiters, not any old quote-mark that might be within the semantic text\n- somehow we got a string that isn't delimited, weird but okay\n- somehow the triple quoted string hasn't been terminated, so don't clip the right side\n- somehow the single quoted string hasn't been terminated, so don't clip the right side\n- Because the work is a little complex, we have a catch-all return of the unadulterated text - just in case someone decides to add a bug in the code laters...",
    "reference": "(76, 4)"
  },
  "lexicographics.LEXICOGRAPHICS._is_expo": {
    "category": "SKILL",
    "canonical": "lexicographics.LEXICOGRAPHICS._is_expo",
    "content": "Determines if a text IS semantic",
    "reference": "(118, 4)"
  },
  "lexicographics.LEXICOGRAPHICS.is_prose_transition": {
    "category": "DISPOSITION",
    "canonical": "lexicographics.LEXICOGRAPHICS.is_prose_transition",
    "content": "Detects transitions into or out of prose blocks",
    "reference": "(127, 4)"
  },
  "lexicographics.LEXICOGRAPHICS.extend_content": {
    "category": "SKILL",
    "canonical": "lexicographics.LEXICOGRAPHICS.extend_content",
    "content": "Extends the latest semantic with additional (prose) commentary.\n\n  BUT if we somehow found a prose block before ANY other semantic, we will ttry to add the prose as its own semantic\n\n  This is so we at least get to see the (mis-placed) element somewhere in the outputs, so we can fix it.\n\n  Note, in this case it truly IS mis-placed, because (by defintion) prose annotates a previous semantic.\n\n  Mis-placed prose in module B could even turn up annotating the last semantic of module A!\n\n  The only defence I offer is that you at least get to see the prose SOMEWHERE...\n\n  \n\n  ...unless you don't. A mis-placed prose block will not be added at all IF it would overwrite an existing semantic.\n\n  This ought to be an impossible scenario, thus the silent use of 'pass' in this code.\n\n  FWIW: I'm sorry, sooooo sorry, if that ever trips you up ;^D",
    "reference": "(144, 4)"
  },
  "lexicographics.LexicalOccurence": {
    "category": "KNOWLEDGE",
    "canonical": "lexicographics.LexicalOccurence",
    "content": "holds an attestation contextualised lexical entity",
    "reference": "(172, 0)"
  },
  "lexicographics.Lexeme": {
    "category": "KNOWLEDGE",
    "canonical": "lexicographics.Lexeme",
    "content": "holds a lexeme - the canonical occurence, category and semantic content of a lexical",
    "reference": "(210, 0)"
  },
  "lexicographics.Lexeme.from_parts": {
    "category": "MECHANISM",
    "canonical": "lexicographics.Lexeme.from_parts",
    "content": "Creates a lexeme by extracting category from a semantic text",
    "reference": "(218, 4)"
  },
  "lexicographics.Lexeme.summary": {
    "category": "SKILL",
    "canonical": "lexicographics.Lexeme.summary",
    "content": "Summarises a lexeme to category and canonical reference",
    "reference": "(230, 4)"
  },
  "lexicographics.Lexeme._dedent": {
    "category": "MECHANISM",
    "canonical": "lexicographics.Lexeme._dedent",
    "content": "The discovered semantics are indented, partly due to the requirements of the originating code, and partly for semantic clarity.\n\nHere we remove the common margin (minimum indent) found within the semantic content.",
    "reference": "(238, 4)"
  },
  "narrate": {
    "category": "WORLD",
    "canonical": "narrate",
    "content": "The purpose here is just to exercise the concept of marrying narrative (story) to source code.\n\n\n\nThe underlying concept is that narrative is a foundational aspect of human cognition - it is through story that we get to experience the abstract; for understanding arises not just from knowing, but from experiencing (feeling) that which is known.\n\n\n\nWe believe coding is a creative process, but to be so the code must ignite cognition.\n\n\n\nThis is the fundamental tenet of the Narratival-Exposition Paradigm, which we explore with this code base.\n\n\n\nSo, this code base reads this code base to produce this code base's documentation...\n\n\n\n## Pre-requisites\n\nThese scripts require a code base that has been written in the (evolving) Narratival-Exposition's grammar - e.g. this code base!\n\n\n\n## World View\n\nGenerating the narrative happens in 3 phases (well, 3 phases after actually writing the code):\n\n- Extracting points of story; textual commentary attached to Pythonic objects. \n\n- Editorialisation of their order (i.e. so that the narrative is orthogonal to the code architecture)\n\n- Narration: pouring the story points into the editorialsation to generate the narrative arc\n\n\n\nHerein, we see the narration of the phase 1 code base: the extraction process. This produces the files needed for the (manual) editorialisation phase, which then allows the narration script to produce the doccumentation.\n\n\n\nDuring extraction the core concepts we will meet are:\n\n- CODICES: books or lore that offer symbolic overlays to the source code\n\n- GRANULATION: the powderisation, purification, mixing and refinement of the codified symbolism\n\n- REGISTRAR: of births, deaths and marriages; providing the lineage (Pythonic scope) of granular entities\n\n- LEXICOGRAPHICS: the mechanisms that sift and convert the lineage-tracked symbolic grains to generate lexemes: the discovered lexical tokens in the source code along with their canonical reference and semantic meaning.",
    "reference": "(1, 0)"
  },
  "narrate.sys": {
    "category": "CONTINUUM",
    "canonical": "narrate.sys",
    "content": "to get CLI args and issue exit status",
    "reference": "(31, 0)"
  },
  "narrate.os": {
    "category": "CONTINUUM",
    "canonical": "narrate.os",
    "content": "for directory walking, and joining strings as paths",
    "reference": "(33, 0)"
  },
  "narrate.pathlib": {
    "category": "CONTINUUM",
    "canonical": "narrate.pathlib",
    "content": "to make o/s independant path from string",
    "reference": "(35, 0)"
  },
  "narrate.granulator": {
    "category": "THROUGHLINE",
    "canonical": "narrate.granulator",
    "content": "A lash-up script in-lieu of real workflow support.\n\nNo metaphor here! We're just porviding scafolding for the workhorse narrate scripts.",
    "reference": "(38, 0)"
  },
  "narrate.all_expositions": {
    "category": "KNOWLEDGE",
    "canonical": "narrate.all_expositions",
    "content": "An initially empty dictionary that comes to hold the full linguistic set as PPython scrippt files are processed",
    "reference": "(46, 0)"
  },
  "narrate.scan_files": {
    "category": "BEHAVIOUR",
    "canonical": "narrate.scan_files",
    "content": "Seeks out files of interest that are then granulated so that expositions can be extracted into the full linguistic set.\n\n\n\n- During extraction the lexicographer is stateful, so we create an instance for it - BUT once we have the expositions for a given script we no longer need that state (since expositions are collated here) so we re-use the instance for each script.\n- We walk sub-directories, excluding those that start with underscore which are porbs holding areas for regressions etc...\n- We scan for Python scripts that do not start with underscore, since they're probably opaque suport files or some kind of transient\n- We create a GRANULATOR instance for each file, but once we have the granulate we don't need it anymore - so these are just transient objects.\n- Once we have the granulate we employ the lexicographer to extract a dictionary of lexemes for this file...\n- ...which we collate into our master dictionary\n- Once all files have been processed we get the LEXICOGRAPHER to list and save the full set of extracted lexemes",
    "reference": "(49, 0)"
  },
  "narrate.confirm_overwrite": {
    "category": "MECHANISM",
    "canonical": "narrate.confirm_overwrite",
    "content": "Just like one of those annoying 'are you sure?' prompts that we all end up regretting just saying 'YES' to one day...",
    "reference": "(95, 0)"
  },
  "narrate.tell_the_tale": {
    "category": "BEHAVIOUR",
    "canonical": "narrate.tell_the_tale",
    "content": "Nothing fancy here, scopes out the scene and tells the tale of any found scripts\n\n\n\n- Are we sitting comfortably? Do we know who's story we are telling, and where we are recording it?\n- Did we tell this tale before and are we happy to overwrite or update it?\n- Then we will begin...",
    "reference": "(103, 0)"
  },
  "registrar": {
    "category": "THROUGHLINE",
    "canonical": "registrar",
    "content": "Every token of interest from the parse (i.e. those that survived the granulator's purification stage) visits the Registrar \u2014 which may seem draconian, but such is the nature of symbolic governance.\n\n\n\nThe Registrar\u2019s records are private. The only sanctioned access is through `record_history`, which returns the current known lineage for each notable recorded subject.\n\n\n\nMost subjects are notable and have their lineage recorded.\n\nThough draconian, the process remains relatively democratic.\n\n\n\nUnnotables are the DENTS.\n\nThe Registrar must still track them, as a DEDENT may signal the end of a lineage \u2014 depending on the INDENTS that preceded it.\n\n\n\nLineage only extends when a true identity is married to a progenitor (`class` or `def` + name).\n\nThe resilience of this family line then waxes and wanes, governed by INDENTS and DEDENTS.\n\n\n\nWhen resilience falls back to the level of the originating progenitor, the family line dies out, and the current lineage contracts.\n\n\n\nLineage is NOT returned for growth/decline subjects (DENTS), as they are not actual 'things' \u2014 just indicators of resilience.\n\n\n\nSimilarly, lineage is NOT returned for honourifics, since they merely address things, but are not things themselves.",
    "reference": "(2, 0)"
  },
  "registrar.REGISTRAR": {
    "category": "FIGURATION",
    "canonical": "registrar.REGISTRAR",
    "content": "The Registrar recognises and declares subject titles according to the evolving lineage of recorded subjects.\n\n\n\nSome subjects (DENTS and PROGENITORS) do not receive titles, but they influence the shape and continuity of the lineage.\n\n\n\nOther subjects (HONOURIFICS) are not recorded at all \u2014 they serve only to address true subjects, and are thus excluded from lineage.",
    "reference": "(24, 0)"
  },
  "registrar.REGISTRAR.__init__.self._resilience": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.REGISTRAR.__init__.self._resilience",
    "content": "tracks how the heritage line waxes and wanes",
    "reference": "(34, 8)"
  },
  "registrar.REGISTRAR.__init__.self._register": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.REGISTRAR.__init__.self._register",
    "content": "A new register is created for each registrant (grand forebear)",
    "reference": "(37, 8)"
  },
  "registrar.REGISTRAR.__init__.self._heir_apparent": {
    "category": "DISPOSITION",
    "canonical": "registrar.REGISTRAR.__init__.self._heir_apparent",
    "content": "are we currently looking for the true identity of an heir apparent, or else just recording subject titles",
    "reference": "(41, 8)"
  },
  "registrar.REGISTRAR.__init__.self._heir": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.REGISTRAR.__init__.self._heir",
    "content": "the true identity of an heir apparent",
    "reference": "(43, 8)"
  },
  "registrar.REGISTRAR.record_history": {
    "category": "BEHAVIOUR",
    "canonical": "registrar.REGISTRAR.record_history",
    "content": "Provides the current lineage relevant to a subject, IF this is a notable subject.\n\n  But NOTE - what a topsy-turvey world we live in!\n\n  Because we have to look forwards we find new progenitors before we find their identity!\n\n  So instead of simply saying: \n\n  > Prepare for heir, seek heir, record heir...\n\n  we have to say:\n\n  > record (the current) heir, seek (another) heir, prepare for (next) heir\n\n  \n\n  Kind of backwards really, I guess it stems from 'invasion culture' wherein conquered peoples are dehumanised by being reduced to 'station' afore 'identity', m'lord!\n\n on how the code-tree grows and withers\n\n- First, keep an eye on the resilience of the current family line\n- Add any found heir to the lineage\n- So that GOING FORWARD the title is recognised\n- Once we find a new heir, keep it safe for now\n- So the title is awarded on the next cycle\n- Otherwise we lose sight of this heir's own lineage\n- Have a look-see if we have met a new heir-apparent\n- Finally all are remembered in the trace of lineage they leave behind \n- BUT it is only the true that get entitled",
    "reference": "(46, 4)"
  },
  "registrar.REGISTRAR._lineage_fluxed": {
    "category": "SKILL",
    "canonical": "registrar.REGISTRAR._lineage_fluxed",
    "content": "Detects flux in the current family line, signing-off the register if the line has died out",
    "reference": "(89, 4)"
  },
  "registrar.REGISTRAR._seek_heir_apparent": {
    "category": "DISPOSITION",
    "canonical": "registrar.REGISTRAR._seek_heir_apparent",
    "content": "Switches our disposition from seeking an identity to seeking a progenitor\n\nwhilst setting the current found heir apparent",
    "reference": "(108, 4)"
  },
  "registrar.REGISTRAR._entitle": {
    "category": "SKILL",
    "canonical": "registrar.REGISTRAR._entitle",
    "content": "Joins up all the identities in our current lineage to form a single, recordable, title",
    "reference": "(121, 4)"
  },
  "registrar.REGISTRAR._prepare_for_heir": {
    "category": "DISPOSITION",
    "canonical": "registrar.REGISTRAR._prepare_for_heir",
    "content": "Prepares a progenitor's new lineage, in case there is then a marriage\n\nSwitches our disposition from seeking a progenitor to seeking an identity",
    "reference": "(129, 4)"
  },
  "registrar.REGISTRAR._record_heir": {
    "category": "SKILL",
    "canonical": "registrar.REGISTRAR._record_heir",
    "content": "Fills in the new lineage, and sets the baseline of this family line's resilience,\n\nwhich builds upon that of previous generations",
    "reference": "(139, 4)"
  },
  "registrar.REGISTRAR._sign_off_record": {
    "category": "SKILL",
    "canonical": "registrar.REGISTRAR._sign_off_record",
    "content": "Marks the end of a family line by removing its lineage",
    "reference": "(151, 4)"
  },
  "registrar.REGISTRAR._register_empty": {
    "category": "FLAW",
    "canonical": "registrar.REGISTRAR._register_empty",
    "content": "Allows us to check if the register is empty before we try to remove a family line",
    "reference": "(158, 4)"
  },
  "registrar.LINEAGE": {
    "category": "AFFORDANCE",
    "canonical": "registrar.LINEAGE",
    "content": "Casts the symbolic LEXICON of the base CODEX into lineage parlance, allowing us to identify lineage related ENTITIES",
    "reference": "(166, 0)"
  },
  "registrar.LINEAGE.PROGENITORS": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.PROGENITORS",
    "content": "The subjects that (potentially) start a new generation in the lineage",
    "reference": "(171, 4)"
  },
  "registrar.LINEAGE.HONOURIFICS": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.HONOURIFICS",
    "content": "The ways in which subjects may be addressed, not actual subject identities",
    "reference": "(174, 4)"
  },
  "registrar.LINEAGE.IDENTITIES": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.IDENTITIES",
    "content": "The subjects that may be true identities",
    "reference": "(177, 4)"
  },
  "registrar.LINEAGE.DESCENDERS": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.DESCENDERS",
    "content": "The subjects that give rise to descendents",
    "reference": "(180, 4)"
  },
  "registrar.LINEAGE.GROWTH": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.GROWTH",
    "content": "Represents a waxing in the current family-line's resilience",
    "reference": "(183, 4)"
  },
  "registrar.LINEAGE.DECLINE": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.DECLINE",
    "content": "Represents a waning in the current family-line's resilience",
    "reference": "(186, 4)"
  },
  "registrar.LINEAGE.TRUE_SUBJECTS": {
    "category": "KNOWLEDGE",
    "canonical": "registrar.LINEAGE.TRUE_SUBJECTS",
    "content": "True subjects - the things we want to record",
    "reference": "(189, 4)"
  },
  "registrar.LINEAGE.is_true_subject": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.is_true_subject",
    "content": "Matches with the subjects we want to record",
    "reference": "(193, 4)"
  },
  "registrar.LINEAGE.is_progenitor": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.is_progenitor",
    "content": "Matches progenitor type subjects only",
    "reference": "(201, 4)"
  },
  "registrar.LINEAGE.is_descender": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.is_descender",
    "content": "Matches descender type subjects only",
    "reference": "(209, 4)"
  },
  "registrar.LINEAGE.is_true_identity": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.is_true_identity",
    "content": "Matches subjects that are true identities (not honourifics)",
    "reference": "(217, 4)"
  },
  "registrar.LINEAGE.growth": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.growth",
    "content": "Detects when the current family line grows",
    "reference": "(227, 4)"
  },
  "registrar.LINEAGE.decline": {
    "category": "SKILL",
    "canonical": "registrar.LINEAGE.decline",
    "content": "Detects when a current family line has declined",
    "reference": "(235, 4)"
  },
  "registrar.LINEAGE.subject_name": {
    "category": "MECHANISM",
    "canonical": "registrar.LINEAGE.subject_name",
    "content": "we DO need to know the actual name of a subject!",
    "reference": "(243, 4)"
  }
}